I"§Ü<h2 id="project-summary">Project Summary</h2>
<p>Financial fraud is a major concern in banking and financial services and costs millions of dollars every year. Credit card fraud continues to rise due to highly imbalanced dataset and non-stationary nature of data. It is estimated that some organization have losses up to 5% due to fraud and recent studies point that fraud activities have risen and expected to increase in future. Losses to financial institution can be avoided by detecting credit card fraud and alerting banks about potential fraudulent transactions.</p>

<p>The first step in building a model is to get the dataset. One of the main issues with credit card fraud detection is unavailability of real dataset. Since the credit card data will have sensitive details about the customer, there are not many datasets available due to privacy issues. Another challenge with credit card dataset is that it is highly imbalanced where there are more legitimate transactions and few fraudulent transactions. Millions of transactions are processed every day and the size of the dataset will be huge. Feature selection technique is applied to use only required features and machine learning algorithm using Random Forest Classifier is used to evaluate and effectively predict fraudulent transactions.</p>

<h3 id="problem-statement">Problem Statement</h3>
<p>In financial industry, fraud is a major threat which leads in financial loss. Credit card fraud results in millions of dollars loss to the banks and/or financial institution. Fraudsters find new ways to conduct fraud in credit card transaction. Losses to financial institution can be avoided by detecting credit card fraud and alerting banks about potential fraudulent transactions.</p>

<h3 id="proposal">Proposal</h3>
<p>By using credit card fraud dataset, firstly I will perform some graph analysis to understand some trends in the dataset. The dataset may be imbalanced which might result in false positive or false negative fraud detection. Hence may need to perform scaling on the dataset. I will be using machine learning algorithms to detect the fraudulent transactions.</p>

<h3 id="dataset">Dataset</h3>
<p>The dataset is selected from Kaggle.
Credit card fraud data - <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">Data source</a></p>

<h4 id="data-variables">Data Variables:</h4>
<p>Columns V1 to V28 are result of Principal Component Analysis (PCA) which has been done due to data compliance.
Amount is the Transaction amount 
Class 0 indicates non-fraud and 1 indicates fraud
Time is the time elapsed between each transaction</p>

<p>Data pre-processing was performed to check for null-values and Exploratory Data Analysis (EDA) was performed.</p>

<p>Bar plot of count of fraud vs non-fraud transactions in the dataset</p>

<p><img src="/img/posts/credit-card/output_11_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Seperate fraud and non-fraud for histogram
</span><span class="n">frd_class</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">non_frd_class</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Histogram of transaction times in fradulent and non-fradulent transactions
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">frd_class</span><span class="p">.</span><span class="n">Time</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Fradulent transaction times"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">non_frd_class</span><span class="p">.</span><span class="n">Time</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Non-fradulent transaction times"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x148cd69b0c8&gt;
</code></pre></div></div>

<p><img src="output_13_1.png" alt="png" /></p>

<p>From above, fradulent transaction shows couple of peak times but it can be observed that time of the trasaction cannot be considered much in the analysis to determine if the transaction is fraudlent or not.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train and test data
</span><span class="n">X</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">"Time"</span><span class="p">,</span><span class="s">"Class"</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="s">"columns"</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">Class</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="o">=</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="p">.</span><span class="mi">2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(227845, 29)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Feature selection using Variance Threshold with threshold of 0.5
</span><span class="n">var</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">var</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">X_train_var</span><span class="o">=</span><span class="n">var</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_var</span><span class="o">=</span><span class="n">var</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train_var</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(227845, 23)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Alternate way to perform feature selection and display the features
</span><span class="k">def</span> <span class="nf">variance_threshold_selector</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">selector</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="p">)</span>
    <span class="n">selector</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">selector</span><span class="p">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="bp">True</span><span class="p">)]]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">variance_threshold_selector</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V14</th>
      <th>V15</th>
      <th>V16</th>
      <th>V17</th>
      <th>V18</th>
      <th>V19</th>
      <th>V20</th>
      <th>V21</th>
      <th>V22</th>
      <th>Amount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>223361</th>
      <td>1.955041</td>
      <td>-0.380783</td>
      <td>-0.315013</td>
      <td>0.330155</td>
      <td>-0.509374</td>
      <td>-0.086197</td>
      <td>-0.627978</td>
      <td>0.035994</td>
      <td>1.054560</td>
      <td>-0.030441</td>
      <td>...</td>
      <td>-0.253266</td>
      <td>-0.331695</td>
      <td>0.307252</td>
      <td>-0.930844</td>
      <td>0.651666</td>
      <td>0.167987</td>
      <td>-0.125390</td>
      <td>0.238197</td>
      <td>0.968305</td>
      <td>9.99</td>
    </tr>
    <tr>
      <th>165061</th>
      <td>-0.400975</td>
      <td>-0.626943</td>
      <td>1.555339</td>
      <td>-2.017772</td>
      <td>-0.107769</td>
      <td>0.168310</td>
      <td>0.017959</td>
      <td>-0.401619</td>
      <td>0.040378</td>
      <td>0.611115</td>
      <td>...</td>
      <td>-1.193347</td>
      <td>0.631053</td>
      <td>-0.160123</td>
      <td>-1.630444</td>
      <td>2.106866</td>
      <td>-1.692780</td>
      <td>-0.470372</td>
      <td>-0.153485</td>
      <td>0.421703</td>
      <td>45.90</td>
    </tr>
    <tr>
      <th>238186</th>
      <td>0.072509</td>
      <td>0.820566</td>
      <td>-0.561351</td>
      <td>-0.709897</td>
      <td>1.080399</td>
      <td>-0.359429</td>
      <td>0.787858</td>
      <td>0.117276</td>
      <td>-0.131275</td>
      <td>-0.638222</td>
      <td>...</td>
      <td>-0.640249</td>
      <td>-0.801946</td>
      <td>0.678131</td>
      <td>0.044374</td>
      <td>0.521919</td>
      <td>0.198772</td>
      <td>0.012227</td>
      <td>-0.314638</td>
      <td>-0.872959</td>
      <td>11.99</td>
    </tr>
    <tr>
      <th>150562</th>
      <td>-0.535045</td>
      <td>1.014587</td>
      <td>1.750679</td>
      <td>2.769390</td>
      <td>0.500089</td>
      <td>1.002270</td>
      <td>0.847902</td>
      <td>-0.081323</td>
      <td>0.371579</td>
      <td>0.560595</td>
      <td>...</td>
      <td>1.271254</td>
      <td>-1.011647</td>
      <td>1.458600</td>
      <td>-0.613260</td>
      <td>0.814931</td>
      <td>-2.147124</td>
      <td>-0.253757</td>
      <td>0.063525</td>
      <td>0.443431</td>
      <td>117.44</td>
    </tr>
    <tr>
      <th>138452</th>
      <td>-4.026938</td>
      <td>1.897371</td>
      <td>-0.429786</td>
      <td>-0.029571</td>
      <td>-0.855751</td>
      <td>-0.480406</td>
      <td>-0.435632</td>
      <td>1.313760</td>
      <td>0.536044</td>
      <td>1.221746</td>
      <td>...</td>
      <td>0.187685</td>
      <td>-1.060579</td>
      <td>0.143332</td>
      <td>0.007803</td>
      <td>-0.055817</td>
      <td>0.712695</td>
      <td>-0.012320</td>
      <td>-0.480691</td>
      <td>-0.230369</td>
      <td>25.76</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>119879</th>
      <td>1.173488</td>
      <td>0.100792</td>
      <td>0.490512</td>
      <td>0.461596</td>
      <td>-0.296377</td>
      <td>-0.213165</td>
      <td>-0.165254</td>
      <td>0.119221</td>
      <td>-0.114199</td>
      <td>0.079128</td>
      <td>...</td>
      <td>0.600937</td>
      <td>0.658885</td>
      <td>0.239566</td>
      <td>-0.356782</td>
      <td>-0.410923</td>
      <td>-0.182494</td>
      <td>-0.157534</td>
      <td>-0.186027</td>
      <td>-0.574283</td>
      <td>1.98</td>
    </tr>
    <tr>
      <th>259178</th>
      <td>-0.775981</td>
      <td>0.144023</td>
      <td>-1.142399</td>
      <td>-1.241113</td>
      <td>1.940358</td>
      <td>3.912076</td>
      <td>-0.466107</td>
      <td>1.360620</td>
      <td>0.400697</td>
      <td>-0.654029</td>
      <td>...</td>
      <td>0.384625</td>
      <td>0.785702</td>
      <td>0.057999</td>
      <td>-0.462528</td>
      <td>-0.063197</td>
      <td>0.094703</td>
      <td>-0.295730</td>
      <td>0.037078</td>
      <td>-0.019575</td>
      <td>89.23</td>
    </tr>
    <tr>
      <th>131932</th>
      <td>-0.146609</td>
      <td>0.992946</td>
      <td>1.524591</td>
      <td>0.485774</td>
      <td>0.349308</td>
      <td>-0.815198</td>
      <td>1.076640</td>
      <td>-0.395316</td>
      <td>-0.491303</td>
      <td>-0.212753</td>
      <td>...</td>
      <td>-0.019493</td>
      <td>0.690200</td>
      <td>-0.144778</td>
      <td>-0.539887</td>
      <td>-0.068375</td>
      <td>-0.229614</td>
      <td>0.007155</td>
      <td>0.052649</td>
      <td>0.354089</td>
      <td>3.94</td>
    </tr>
    <tr>
      <th>146867</th>
      <td>-2.948638</td>
      <td>2.354849</td>
      <td>-2.521201</td>
      <td>-3.798905</td>
      <td>1.866302</td>
      <td>2.727695</td>
      <td>-0.471769</td>
      <td>2.217537</td>
      <td>0.580199</td>
      <td>-0.027572</td>
      <td>...</td>
      <td>0.927786</td>
      <td>-0.090065</td>
      <td>0.426113</td>
      <td>-0.396148</td>
      <td>-0.946837</td>
      <td>-1.143752</td>
      <td>0.417396</td>
      <td>-0.332759</td>
      <td>-1.047514</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>121958</th>
      <td>1.233174</td>
      <td>-0.784851</td>
      <td>0.386784</td>
      <td>-0.698559</td>
      <td>-1.034018</td>
      <td>-0.637028</td>
      <td>-0.502369</td>
      <td>-0.188057</td>
      <td>-0.749637</td>
      <td>0.543016</td>
      <td>...</td>
      <td>-0.346209</td>
      <td>0.893667</td>
      <td>1.447832</td>
      <td>-0.040565</td>
      <td>-1.314021</td>
      <td>0.673568</td>
      <td>0.337732</td>
      <td>0.027634</td>
      <td>-0.234522</td>
      <td>113.00</td>
    </tr>
  </tbody>
</table>
<p>227845 rows Ã— 23 columns</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">varth_features</span><span class="o">=</span><span class="n">var</span><span class="p">.</span><span class="n">get_support</span><span class="p">()</span>
<span class="n">varth_features</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True, False, False, False, False, False,
       False,  True])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">varth_features</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.image.AxesImage at 0x148be7fd888&gt;
</code></pre></div></div>

<p><img src="output_23_1.png" alt="png" /></p>

<p>Variance threshold is calculated based on probability density function of a particular distribution. 
If a feature has 95% or more variability then is very close to zero and the feature may not help in the model prediciton and it can be removed. The values with True are the features selected using Variance threshold technique. The columns from V23 to V28 are removed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Feature selection using SelectKBest feature selection
</span><span class="n">skbest</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">skbest</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">X_train_skbest</span><span class="o">=</span><span class="n">skbest</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_skbest</span><span class="o">=</span><span class="n">skbest</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train_skbest</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(227845, 10)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kbest_features</span><span class="o">=</span><span class="n">skbest</span><span class="p">.</span><span class="n">get_support</span><span class="p">()</span>
<span class="n">kbest_features</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([False, False,  True,  True, False, False,  True, False, False,
        True,  True,  True, False,  True, False,  True,  True,  True,
       False, False, False, False, False, False, False, False, False,
       False, False])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># SelectKBest to determine 10 best features
</span><span class="n">best_features</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">f_classif</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">best_features</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">df_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fit</span><span class="p">.</span><span class="n">scores_</span><span class="p">)</span>
<span class="n">df_columns</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="c1"># concatenate dataframes
</span><span class="n">feature_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_columns</span><span class="p">,</span> <span class="n">df_scores</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">feature_scores</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Feature_Name'</span><span class="p">,</span><span class="s">'Score'</span><span class="p">]</span>  <span class="c1"># name output columns
</span><span class="k">print</span><span class="p">(</span><span class="n">feature_scores</span><span class="p">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="s">'Score'</span><span class="p">))</span>         <span class="c1"># print 10 best features
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Feature_Name         Score
16          V17  27240.806212
13          V14  23547.660704
11          V12  16985.532943
9           V10  11096.188557
15          V16   9239.509953
2            V3   8425.516128
6            V7   7782.311282
10          V11   5680.321617
3            V4   4173.023409
17          V18   2865.506910
</code></pre></div></div>

<p>The values with True are the features selected using SelectKBest technique. Most relevant 10 features are selected. The features selected can be tested by running throught the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate precision recall area under curve
</span><span class="k">def</span> <span class="nf">preci_auc</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">pred_prob</span><span class="p">):</span>
   <span class="c1"># calculate precision-recall curve
</span>    <span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">pred_prob</span><span class="p">)</span>
    <span class="c1"># calculate area under curve
</span>    <span class="k">return</span> <span class="n">auc</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Evaluate a model
</span><span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="c1"># Define evaluation procedure
</span>    <span class="n">CV</span> <span class="o">=</span> <span class="n">RepeatedStratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Define the model evaluation the metric
</span>    <span class="n">metric</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">preci_auc</span><span class="p">,</span> <span class="n">needs_proba</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># Evaluate model
</span>    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'roc_auc'</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">CV</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scores</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define reference model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">'constant'</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Variance threshold returned 23 features and Iâ€™ll be using this training and test data for further process</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define the reference model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">'constant'</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Evaluate the model
</span><span class="n">scores</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">X_train_var</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="c1"># summarize performance
</span><span class="k">print</span><span class="p">(</span><span class="s">'Mean area under curve: %.3f (%.3f)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mean area under curve: 0.500 (0.000)
</code></pre></div></div>

<p>From above, the baseline score is 0.50. Hence the model selected should be atleast above this score.</p>

<p>Since the values are of PCA transformation, it is better to normalize the data as it could impact the performance of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Normalize the input
</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_var</span><span class="p">)</span>
<span class="n">X_train_norm</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train_var</span><span class="p">)</span>
<span class="n">X_test_norm</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_var</span><span class="p">)</span>
</code></pre></div></div>

<p>Model selection - One of the common models is Logistic regression. Few other models are compared to see the results. Cross validation method is used.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model_val</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">scor</span><span class="p">,</span> <span class="n">show</span><span class="p">):</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

  <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scor</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">show</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Score: {:.2f} (+/- {:.2f})"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">scores</span><span class="p">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">.</span><span class="n">std</span><span class="p">()))</span>
  
  <span class="k">return</span> <span class="n">scores</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># List of models
</span><span class="n">rfc</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">ctc</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">sglc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">()</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">score</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Check model score
</span><span class="k">for</span> <span class="n">classifier</span> <span class="ow">in</span> <span class="p">(</span><span class="n">rfc</span><span class="p">,</span> <span class="n">ctc</span><span class="p">,</span> <span class="n">sglc</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">classifier</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="p">)</span>
    <span class="n">score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_val</span><span class="p">(</span><span class="n">X_train_norm</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">scor</span><span class="o">=</span><span class="s">'roc_auc'</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

<span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">score</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'roc_auc'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Score: 0.94 (+/- 0.02)
Score: 0.88 (+/- 0.03)
Score: 0.98 (+/- 0.01)
Score: 0.98 (+/- 0.01)
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>roc_auc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>RandomForestClassifier</th>
      <td>0.943755</td>
    </tr>
    <tr>
      <th>DecisionTreeClassifier</th>
      <td>0.879323</td>
    </tr>
    <tr>
      <th>SGDClassifier</th>
      <td>0.980483</td>
    </tr>
    <tr>
      <th>LogisticRegression</th>
      <td>0.977406</td>
    </tr>
  </tbody>
</table>
</div>

<p>From above, Random Forest Classifier and Decision Tree Classifier have better scores. I will be working on testing and validating the models in coming week.</p>

<h3 id="random-forest-model-evaluation">Random Forest Model Evaluation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pipeline_rf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">'model'</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="p">])</span>
</code></pre></div></div>

<p>As the time taken to process the large dataset is more, just specified the number of estimators instead of hyperparameter grid search.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parm_gridscv_rf</span> <span class="o">=</span> <span class="p">{</span><span class="s">'model__n_estimators'</span><span class="p">:</span> <span class="p">[</span><span class="mi">75</span><span class="p">]}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid_rf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">pipeline_rf</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">parm_gridscv_rf</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'roc_auc'</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> 
                       <span class="n">pre_dispatch</span><span class="o">=</span><span class="s">'2*n_jobs'</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid_rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_norm</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fitting 5 folds for each of 1 candidates, totalling 5 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.8min remaining:  4.2min
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.8min finished





GridSearchCV(cv=5, error_score=nan,
             estimator=Pipeline(memory=None,
                                steps=[('model',
                                        RandomForestClassifier(bootstrap=True,
                                                               ccp_alpha=0.0,
                                                               class_weight=None,
                                                               criterion='gini',
                                                               max_depth=None,
                                                               max_features='auto',
                                                               max_leaf_nodes=None,
                                                               max_samples=None,
                                                               min_impurity_decrease=0.0,
                                                               min_impurity_split=None,
                                                               min_samples_leaf=1,
                                                               min_samples_split=2,
                                                               min_weight_fraction_leaf=0.0,
                                                               n_estimators=100,
                                                               n_jobs=-1,
                                                               oob_score=False,
                                                               random_state=1,
                                                               verbose=0,
                                                               warm_start=False))],
                                verbose=False),
             iid='deprecated', n_jobs=-1,
             param_grid={'model__n_estimators': [75]}, pre_dispatch='2*n_jobs',
             refit=True, return_train_score=False, scoring='roc_auc',
             verbose=1)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">grid_rf</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean_fit_time</th>
      <th>std_fit_time</th>
      <th>mean_score_time</th>
      <th>std_score_time</th>
      <th>param_model__n_estimators</th>
      <th>params</th>
      <th>split0_test_score</th>
      <th>split1_test_score</th>
      <th>split2_test_score</th>
      <th>split3_test_score</th>
      <th>split4_test_score</th>
      <th>mean_test_score</th>
      <th>std_test_score</th>
      <th>rank_test_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>163.587383</td>
      <td>1.105021</td>
      <td>0.695453</td>
      <td>0.357422</td>
      <td>75</td>
      <td>{'model__n_estimators': 75}</td>
      <td>0.953661</td>
      <td>0.948029</td>
      <td>0.92267</td>
      <td>0.96075</td>
      <td>0.948151</td>
      <td>0.946652</td>
      <td>0.012862</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid_rf</span><span class="p">.</span><span class="n">best_score_</span><span class="p">,</span> <span class="n">grid_rf</span><span class="p">.</span><span class="n">best_params_</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(0.9466523268247682, {'model__n_estimators': 75})
</code></pre></div></div>

<h3 id="test-random-forest-model">Test Random Forest model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">grid_rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_norm</span><span class="p">)</span>
<span class="c1"># Decimal places based on number of samples
</span><span class="n">dec</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))))</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">'Confusion Matrix'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">'Classification report'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="n">dec</span><span class="p">))</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">'Scalar Metrics'</span><span class="p">)</span>
<span class="n">format_str</span> <span class="o">=</span> <span class="s">'%%13s = %%.%if'</span> <span class="o">%</span> <span class="n">dec</span>
<span class="k">if</span> <span class="n">y_test</span><span class="p">.</span><span class="n">nunique</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span> <span class="c1"># metrics for binary classification
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="n">y_score</span> <span class="o">=</span> <span class="n">grid_rf</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_norm</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">y_score</span> <span class="o">=</span> <span class="n">grid_rf</span><span class="p">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X_test_norm</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">format_str</span> <span class="o">%</span> <span class="p">(</span><span class="s">'AUROC'</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Confusion Matrix
[[56861     3]
 [   21    77]] 

Classification report
              precision    recall  f1-score   support

           0    0.99963   0.99995   0.99979     56864
           1    0.96250   0.78571   0.86517        98

    accuracy                        0.99958     56962
   macro avg    0.98107   0.89283   0.93248     56962
weighted avg    0.99957   0.99958   0.99956     56962

Scalar Metrics
        AUROC = 0.95824
</code></pre></div></div>

<h3 id="plot-confusion-matrix">Plot confusion matrix</h3>
<p>skplt.metrics.plot_confusion_matrix(y_test, y_pred)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">log_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.014552386367581962
</code></pre></div></div>

<h3 id="logistic-regression-model-evaluation">Logistic Regression Model Evaluation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Logistic regression model with different C values
</span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'tol'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.00001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">],</span>
    <span class="s">'C'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">lgr</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
                     <span class="n">param_grid</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                     <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                     <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">scoring</span><span class="o">=</span><span class="s">'roc_auc'</span>
                    <span class="p">)</span>
<span class="n">lgr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_norm</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">lgr</span><span class="p">.</span><span class="n">best_estimator_</span>

<span class="k">print</span><span class="p">(</span><span class="n">lgr</span><span class="p">.</span><span class="n">best_estimator_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The best classifier score:"</span><span class="p">,</span><span class="n">lgr</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=1, penalty='l2', random_state=101,
                   solver='lbfgs', tol=1e-05, verbose=0, warm_start=False)
The best classifier score: 0.9753252317330124
</code></pre></div></div>

<h3 id="test-logistic-regression-model">Test Logistic Regression Model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred1</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_norm</span><span class="p">)</span>
<span class="c1"># Decimal places based on number of samples
</span><span class="n">dec</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))))</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">'Confusion Matrix'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred1</span><span class="p">),</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">'Classification report'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred1</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="n">dec</span><span class="p">))</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">'Scalar Metrics'</span><span class="p">)</span>
<span class="n">format_str</span> <span class="o">=</span> <span class="s">'%%13s = %%.%if'</span> <span class="o">%</span> <span class="n">dec</span>
<span class="k">if</span> <span class="n">y_test</span><span class="p">.</span><span class="n">nunique</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span> <span class="c1"># metrics for binary classification
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="n">y_score1</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_norm</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">y_score1</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X_test_norm</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">format_str</span> <span class="o">%</span> <span class="p">(</span><span class="s">'AUROC'</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score1</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Confusion Matrix
[[56855     9]
 [   40    58]] 

Classification report
              precision    recall  f1-score   support

           0    0.99930   0.99984   0.99957     56864
           1    0.86567   0.59184   0.70303        98

    accuracy                        0.99914     56962
   macro avg    0.93248   0.79584   0.85130     56962
weighted avg    0.99907   0.99914   0.99906     56962

Scalar Metrics
        AUROC = 0.98116
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot confusion matrix
</span><span class="n">skplt</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x148dc4b9448&gt;
</code></pre></div></div>

<p><img src="output_58_1.png" alt="png" /></p>

<p>Project code is present in Github - <a href="https://github.com/santosh0924/Credit-Card-Fraud-Detection">Project Link</a></p>
:ET