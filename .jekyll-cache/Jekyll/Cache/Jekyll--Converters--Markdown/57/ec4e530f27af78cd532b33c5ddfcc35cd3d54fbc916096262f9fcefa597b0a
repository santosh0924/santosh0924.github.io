I"<h2 id="project-summary">Project Summary</h2>
<p>‘Fake News’ term is used for intentional misrepresentation of news. They are false stories that are communicated through print, television, and social media. Fake news can be easily spread, and it can be on different topics targeting person or institution. The motive to spread fake news is to mislead the readers and harm reputation of person or organization. Some people or organizations use fake news to project themselves and gain from that sensational news. Analysis on fake news show that although complete fake news is not effective, small portion of them is much effective. They are increasingly prominent and draw more attention than actual facts. Fake news spread very rapidly in social media and reaches all over the world.</p>

<p>The project proposal to detect the fake news so that people do not believe it and share with others. As part of this project, the ‘Fake News’ dataset will be analyzed to predict if the news is reliable or Fake (Unreliable). Preprocessing will be done to clean the data and Natural Language Processing (NLP) techniques will be used to convert the text data which can be used for machine learning models. Exploratory Data Analysis (EDA) will be performed for data visualization. Dataset will be split into train and test data. Machine learning models like Linear Regression, Random Forest will be applied, and performance will be evaluated. The model with best score will be selected and tuned to improve performance.</p>

<h2 id="problem-statement">Problem Statement</h2>
<p>Fake news has been around for long time and with the advent of online platforms and smart phones, users can easily access the news. It provides an opportunity for cyber criminals to spread fake news. The fake news spread can be harmful for individual or society. The readers could believe them without any verification. It is challenging for people to identify whether the news is authentic or not. Some websites contain articles and posts that are purposely created and inaccurate. For example, the fake news is spread by having website name close to the authentic one and close look would reveal the website itself is not authentic.</p>

<p>Fake news should be detected and controlled early so it is not spread to many. People’s opinion can change with fake news. It was reported that during 2016 US elections, misrepresentation of actual stories was widely spread about many candidates. The fake news spread can have impact on political results. In some cases, it can cause anxiousness among people and pose threat to public health. Posts and images are circulated in social media and when viewed from outside of the scope, can be perceived as accurate.</p>

<p>The text data from Kaggle is used for analysis to detect if the news is fake. Machine learning algorithms are used, scores of few models are compared and best model is selected.</p>

<h3 id="dataset">Dataset</h3>
<p>The dataset is selected from Kaggle.
Credit card fraud data - <a href="https://www.kaggle.com/c/fake-news/data">Data source</a></p>

<h2 id="data-exploration">Data Exploration</h2>
<p>The fake news dataset is selected from Kaggle. The first thing is to look at different variables available and understand them. The dataset has 5 variables, they are –
Id – It is the sequential number for records
Title – is the title of the text data
Author – author of the text data
Text – is the actual text or news
Label – is the predictor variable that contains value 0 and 1. 0 is reliable news data and 1 is fake new data.</p>

<p>The dataset is processed to replace null values with spaces. News data is present in ‘text’. Variable ‘author’ contains the author of the text. It is good to consider author and text for processing since considering more words would result in better detection. Hence, the ‘text’ and ‘author’ variable values are combined.</p>

<h2 id="natural-language-processing">Natural Language Processing</h2>
<p>Natural Language Processing or NLP is the technology used to help computer understand human language. The information from text is extracted so that it could be used machine learning algorithms. Natural Language Toolkit (NLTK) is used for NLP processing. NLTK is one of the common and powerful python libraries that contains packages to convert normal text to machine readable format.</p>

<p>Once the data is pre-processed, NLP techniques are used. Firstly, all the text is converted to lower case. Special characters are then removed. Numbers in the text impacts the score when used with machine learning model. Hence, numbers in text are also removed. Stop words are then removed. Stop words are words like ‘a’, ‘an’, ‘the’ that are commonly used in our natural language. These
words need to be removed as it does not add any value to the text. It occupies spaces and processing time is impacted.</p>

<p>Stemming is then performed on the text. Stemming process reduces words to their base or root form. NLTK Portstemmer is used for stemming. Lemmatization is alternate technique used for stemming. Portstemmer is selected as it takes less time for processing compared to lemmatization.</p>

<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2>

<p>The predictor variable ‘label’ is visualized to check for records.</p>

<p><img src="/img/posts/fakenews/image1.png" alt="png" /></p>

<p>The character count and word count of text is then visualized. Histogram is used for the same. 
Below is character count on reliable news and fake news -</p>

<p><img src="/img/posts/fakenews/image2.png" alt="png" />
<img src="/img/posts/fakenews/image3.png" alt="png" /></p>

<p>Below is word count on reliable news and fake news -</p>

<p><img src="/img/posts/fakenews/image4.png" alt="png" />
<img src="/img/posts/fakenews/image5.png" alt="png" /></p>

<h2 id="model-selection-and-evaluation">Model Selection and Evaluation</h2>
<p>The text data after the stemming process is converted to numerical values using Term Frequency-Inverse Document Frequency (TF-IDF). In TF-IDF, the frequency of the words in the text is calculated instead of just counting the words. The tfidf transform function is used convert text to normalized tfidf representation. Since the dataset has around 20k records, the words are more and during TF-IDF process and there was memory issue as the system could not handle those many words when used with machine learning models. Hence, max_features option available in TF-IDF to set it to 1000.</p>

<p>After the data is transformed, the data is split into train and test. 80% is selected for training and 20% is selected for testing. The train data is fit with different machine learning models. ROC AUC (receiver operating characteristic curve) metric is used for model evaluation.
The models used are Logistic Regression, Decision Tree Classifier, SGD classifier, Random Forest and Naïve Bayes. Random Forest model has the best ROC-AUC score of 98.27%.</p>

<p>Random Forest is selected for further performance tuning. Random Forest model is tuned with hyper parameters using Grid Search CV. The Grid SearchCV on Random Forest classifier improved the ROC-AUC score to 98.42%.</p>

<p><img src="/img/posts/fakenews/image6.png" alt="png" /></p>

<p>Project code is present in Github - <a href="https://github.com/santosh0924/Fake-News-detection-using-Machine-Learning">Project Link</a></p>
:ET